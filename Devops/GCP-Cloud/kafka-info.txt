3) What is kafka?
A) Apache kafka is a distributed event streaming platform that is used for building real time data pipelines and streaming applications.



kafka is a Real time Processing with Stream API . The stream API allows you to create real-time apps by continuously transforming and analyzing incoming data streams.

kafka streams API is a librabry you embed in your app to perform stream processing.

example : - driver location updates in uber or other platforms.
sales updates in e-commerce 


kafka port is 9092

Producers are nothing but some one the services that produce the events to the topic.
Once the data is lands to the topic the other data consumes and produce some other relatable results.

Kafka Broker - The foundation of message storage
A kafka broker stores and manages messages in a cluster.

message management - Brokers handle storing , retrieving and distributing messages.

clusternode - Each broker is a node in the kafka cluster

scalability - Brokers enable horizonal scalling by distributing data across nodes.
fault tolerance - redundancy ensures message durability during broker failures.
Brokers can join or leave the clusters without downtime.

Understanding Kafka Topics
*************************
brokers will store all data 
A topic organizes and categorizes kafka messages.

messages in a topic are stored sequenctialy 
Multiple consumers can read from the same topic. - all consumers will pull the data based on the position.

Producers and consumers interact through topics, not directly.

Repication : - Topics ensures data availability with replication across brokers.

retention : this config is to ensure how long the data should be available 


High availability of kafka using replication and partitions 
***********************************************************

Distribution : -  Partitioning distributes a topic's data across multiple brokers in a kafka cluster.
parallelisim : -  partitions enable parallel processing of messages by consumers increasing throughput.


The Creation Process
When you create a topic (e.g., my_topic) with a certain number of partitions (e.g., 6) and a replication factor (e.g., 3), the following happens:

Partition Distribution: The 6 partitions of my_topic are distributed and created across the 3 brokers in your cluster. Kafka's controller broker handles this distribution, ensuring the partitions are spread out as evenly as possible.

Replication: The replication factor of 3 means that each partition has three copies (replicas). One of these is designated as the leader, and the others are followers. All writes and reads for a given partition go through its leader. The followers asynchronously replicate the data from the leader.

commands to create topic , partitions and replicas 
****************************************************

cd /root/kafka/bin ---- shell script of kafka will be placed in this folder.

./kafka-topics.sh --create --topic demo_topic --bootstrap-server localhost:9092

create a topic with partition and replication

./kafka-topics.sh --create --topic demo_topic1 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092

default partition is 1 

to read the configuration of topic 

./kafka-topics.sh --describe --topic demo_topic1 --bootstrap-server localhost:9092

In Kafka, ISR stands for "In-Sync Replicas." It's a critical concept for ensuring data durability and high availability.

Each partition in a Kafka topic has one replica designated as the leader, which handles all read and write requests. The other replicas are followers. The ISR is a dynamic set that includes the leader and all followers that have fully caught up with the leader's data.

The ISR is important for two main reasons:

Data Safety: A message is only considered "committed" when all replicas in the ISR have successfully replicated it. This prevents data loss if the leader fails, because another replica in the ISR will have a complete copy of the data.

Fault Tolerance: When a leader broker fails, Kafka will elect a new leader from the replicas currently in the ISR. This ensures that the new leader has the most up-to-date data, minimizing any potential data loss.

if any broker is out of syc , to fix that you need to reassighn the partion.

Create test.json

{
  "version": 1,
  "partitions": [
    {
      "topic": "my-topic",
      "partition": 1,
      "replicas": [
        102,
        103,
        101
      ]
    }
  ]
}

execute below command 

bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file test.json --execute

To change retention config to the topic 

./kafka-config.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name demo_topic1 --alter --add-config retention.ms=17564335

to list numbers of brokers and topics 

bin/kafka-topics.sh --list --bootstrap-server <broker-address:port>

bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092


zookeeper:
**********

Based on your previous queries about Kafka, ZooKeeper is an essential component for older Kafka versions (before Kafka 3.x with KRaft).

ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. In the context of Kafka, it serves as the cluster's "source of truth."

Here's a breakdown of its primary roles:

Controller Election: ZooKeeper is used to elect the Kafka cluster controller, which is responsible for managing the state of partitions and replicas.

Broker Registration: Every Kafka broker registers itself with ZooKeeper, allowing the controller and other brokers to know which brokers are currently available.

Topic and Partition Metadata: It stores critical metadata about topics, partitions, and their replicas, including which broker is the leader for each partition and the current members of the In-Sync Replica (ISR) set.

ACLs and Quotas: ZooKeeper is also the default location for storing Access Control Lists (ACLs) and other security-related configurations.

For authentication and authorization
*************************************

Edit server.properties: On each Kafka broker, you must edit the config/server.properties file to define a listener that requires authentication. For example, to enable SASL/PLAIN (username/password authentication), you'd add or modify the listeners and sasl.enabled.mechanisms properties.


to grant ACL permissions for authorization 
******************************************

bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:alice --producer --topic my-secure-topic

bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic my-secure-topic


commands to produce / consume 

./kafka-console-producer.sh --topic --bootstrap-server --property "acks-all"

./kafka-console-consumer.sh --topic --bootsteap-server --from-begining / erleir / offset  --max-messages 100 


kraft - kafka raft 

is a new way of handling co-ordination and metadata without needing zookeeper
it uses a built it raft protocal so that kafka brokers manges evrything themselves.

process to install kraft 

- > Generate a uinque ID
bin/kafka-storage.sh randon-uuid 

- > format the storage directory with the genarated cluster ID 
bin/kafka-storage.sh format -t <UUID> -c config/kraft/server.properties

- > start kafka server 

bin/kafka-server-start.sh config/kraft/server.properties 


example1: Create a topic with 2 partitions and publish message with key so that it distribute the messages among all partitions.
and then create two consumers connected to a topic part of same group.

-> Create a Topic with 2 partitions
./kafka-topics.sh --create --topic orders --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 

-> Start producer and send messages using key so that it will distribute among both partitions.

kafka-console-producer.sh --bootstarp-server localhost:9092 --topic test-topic3 --property parse.key=true --property key.seperator=:

below are test messages after connecting to producer 

keyA:message1
keyB:message2

if keyA meesage is consumed from consumer1 and if we publish message again with with keyA then same consumer will consume again. it will not consume by other consumer.

- > Create two consumers connected to a topic part of same group.

kafka-console-consumer.sh --bootstarp-server localhost:9092 --topic test-topic3 --group tg 


To list out consumer groups 
***************************

./kafka-consumer-groups.sh --bootstarp-server localhost:9092 --describe --group my-group 

./kafka-consumer-groups.sh --bootstarp-server localhost:9092 --list

from the consumer group you can identify which consumer is connected to which topi and which partition to pull data and also we can see the current offset and log end offset and host ip of where consumer ran.

****************************

TO setup kafka UI

- add your IP in server.properties
      advertised.listeners=PLAINTEXT://<IP>:9092
      
  And then restart kafka
  
- use docker image provectuslabs/kafka-ui 

docker run --rm -d -p 8080:8080 -e KAFKA_CLUSTERS_0_NAME=my-cluster KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=<IP>:9092 --name kafkaui --image provectuslabs/kafka-ui



Project1:- 

- Python program which takes user email and store in a text file 
- producer.py 
       it will keep checking the text file if any new user added in the file and send it to kafka topic.
- consumer.py
       It will keep checking any new data in kafka topic and consume the messages.

Note: will add the code in python folder.


*******************

kafka connect :
****************

kafka connect is a tool in apache kafka that helps move data between kafka and other systems. like ( database , cloud storage , message queues ). autometically using connectors .

we have 2 types of connectors 

source connectors -- Pull data from external sources to kafka 

sync connectors  -- send kafka data to external services 

use scripts like connect-distributed.sh or connect-standalone.sh 

kafka stream 
**************

kafka stream is a java librabry for real time stream processing on kafka topics . it allows you to read , transform , aggregate and write data back to kafka -- all within your application.

grant ACL permissions
***********************

for publisher -> ./kafka-acls.sh --add --bootstarp-server localhost:9092 --allow-principal User:user --topic my-topic --operation write

for consumer ->  ./kafka-acls.sh --add --bootstarp-server localhost:9092 --allow-principal User:user --topic my-topic --operation read

for consumer group -> ./kafka-acls.sh --bootstarp-server localhost:9092 --add --allow-principal User:user-name --group my-group --operation read 

To list acls - >  ./kafka-acls.sh --bootstarp-server localhost:9092 --list --topic my-topic


ports :

2181 for zookeeper 
9092 for kafka 










