3) What is kafka?
A) Apache kafka is a distributed event streaming platform that is used for building real time data pipelines and streaming applications.



kafka is a Real time Processing with Stream API . The stream API allows you to create real-time apps by continuously transforming and analyzing incoming data streams.

kafka streams API is a librabry you embed in your app to perform stream processing.

example : - driver location updates in uber or other platforms.
sales updates in e-commerce 


kafka port is 9092

Producers are nothing but some one the services that produce the events to the topic.
Once the data is lands to the topic the other data consumes and produce some other relatable results.

Kafka Broker - The foundation of message storage
A kafka broker stores and manages messages in a cluster.

message management - Brokers handle storing , retrieving and distributing messages.

clusternode - Each broker is a node in the kafka cluster

scalability - Brokers enable horizonal scalling by distributing data across nodes.
fault tolerance - redundancy ensures message durability during broker failures.
Brokers can join or leave the clusters without downtime.

Understanding Kafka Topics
*************************
brokers will store all data 
A topic organizes and categorizes kafka messages.

messages in a topic are stored sequenctialy 
Multiple consumers can read from the same topic. - all consumers will pull the data based on the position.

Producers and consumers interact through topics, not directly.

Repication : - Topics ensures data availability with replication across brokers.

retention : this config is to ensure how long the data should be available 


High availability of kafka using replication and partitions 
***********************************************************

Distribution : -  Partitioning distributes a topic's data across multiple brokers in a kafka cluster.
parallelisim : -  partitions enable parallel processing of messages by consumers increasing throughput.


The Creation Process
When you create a topic (e.g., my_topic) with a certain number of partitions (e.g., 6) and a replication factor (e.g., 3), the following happens:

Partition Distribution: The 6 partitions of my_topic are distributed and created across the 3 brokers in your cluster. Kafka's controller broker handles this distribution, ensuring the partitions are spread out as evenly as possible.

Replication: The replication factor of 3 means that each partition has three copies (replicas). One of these is designated as the leader, and the others are followers. All writes and reads for a given partition go through its leader. The followers asynchronously replicate the data from the leader.

commands to create topic , partitions and replicas 
****************************************************

cd /root/kafka/bin ---- shell script of kafka will be placed in this folder.

./kafka-topics.sh --create --topic demo_topic --bootstrap-server localhost:9092

create a topic with partition and replication

./kafka-topics.sh --create --topic demo_topic1 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092

default partition is 1 

to read the configuration of topic 

./kafka-topics.sh --describe --topic demo_topic1 --bootstrap-server localhost:9092

In Kafka, ISR stands for "In-Sync Replicas." It's a critical concept for ensuring data durability and high availability.

Each partition in a Kafka topic has one replica designated as the leader, which handles all read and write requests. The other replicas are followers. The ISR is a dynamic set that includes the leader and all followers that have fully caught up with the leader's data.

The ISR is important for two main reasons:

Data Safety: A message is only considered "committed" when all replicas in the ISR have successfully replicated it. This prevents data loss if the leader fails, because another replica in the ISR will have a complete copy of the data.

Fault Tolerance: When a leader broker fails, Kafka will elect a new leader from the replicas currently in the ISR. This ensures that the new leader has the most up-to-date data, minimizing any potential data loss.

if any broker is out of syc , to fix that you need to reassighn the partion.

Create test.json

{
  "version": 1,
  "partitions": [
    {
      "topic": "my-topic",
      "partition": 1,
      "replicas": [
        102,
        103,
        101
      ]
    }
  ]
}

execute below command 

bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file test.json --execute

To change retention config to the topic 

./kafka-config.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name demo_topic1 --alter --add-config retention.ms=17564335

to list numbers of brokers and topics 

bin/kafka-topics.sh --list --bootstrap-server <broker-address:port>

bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092


zookeeper:
**********

Based on your previous queries about Kafka, ZooKeeper is an essential component for older Kafka versions (before Kafka 3.x with KRaft).

ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. In the context of Kafka, it serves as the cluster's "source of truth."

Here's a breakdown of its primary roles:

Controller Election: ZooKeeper is used to elect the Kafka cluster controller, which is responsible for managing the state of partitions and replicas.

Broker Registration: Every Kafka broker registers itself with ZooKeeper, allowing the controller and other brokers to know which brokers are currently available.

Topic and Partition Metadata: It stores critical metadata about topics, partitions, and their replicas, including which broker is the leader for each partition and the current members of the In-Sync Replica (ISR) set.

ACLs and Quotas: ZooKeeper is also the default location for storing Access Control Lists (ACLs) and other security-related configurations.

For authentication and authorization
*************************************

Edit server.properties: On each Kafka broker, you must edit the config/server.properties file to define a listener that requires authentication. For example, to enable SASL/PLAIN (username/password authentication), you'd add or modify the listeners and sasl.enabled.mechanisms properties.


to grant ACL permissions for authorization 
******************************************

bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:alice --producer --topic my-secure-topic

bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic my-secure-topic


commands to produce / consume 

./kafka-console-producer.sh --topic --bootstrap-server --property "acks-all"

./kafka-console-consumer.sh --topic --bootsteap-server --from-begining / erleir / offset  --max-messages 100 
